{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4i5n883p_Xf"
   },
   "source": [
    "_Lambda School Data Science — Regression 2_\n",
    "\n",
    "# Doing Linear Regression\n",
    "\n",
    "### Objectives\n",
    "- acquire data for features\n",
    "- arrange data into X features matrix and y target vector\n",
    "- begin with baselines for regression\n",
    "- use scikit-learn for linear regression\n",
    "- use regression metric: MAE\n",
    "- do leave-one-out cross-validation\n",
    "\n",
    "### Contents\n",
    "1. Pre-reads\n",
    "2. Process\n",
    "3. Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6O0KEmmQ7OM"
   },
   "source": [
    "# Pre-reads\n",
    "\n",
    "#### [Jake VanderPlas, Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html). \n",
    "\n",
    "Read up through “Supervised learning example: Simple linear regression”. You can stop when you get to “Supervised learning example: Iris classification.”\n",
    "\n",
    "#### [Nate Silver, What Do Economic Models Really Tell Us About Elections?](https://fivethirtyeight.com/features/what-do-economic-models-really-tell-us-about-elections/)\n",
    "\n",
    "Read the whole thing. We’ll make a model similar to the “Bread and Peace” model...\n",
    "\n",
    ">Perhaps the best-known of these models is the so-called “Bread and Peace” model designed by Douglas Hibbs of the University of Gothenberg. There are a lot of things to admire about this model. Most notably, it’s not larded down with superfluous variables. Instead, it is based on just two: growth in real, per-capita disposable income, and the number of military fatalities resulting from U.S.-initiated foreign conflicts.\n",
    "\n",
    "... and then you’ll make your own elections model, with two features of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZ7Oh030150T"
   },
   "source": [
    "# Process\n",
    "\n",
    "#### Renee Teate, [Becoming a Data Scientist, PyData DC 2016 Talk](https://www.becomingadatascientist.com/2016/10/11/pydata-dc-2016-talk/)\n",
    "\n",
    "![](https://image.slidesharecdn.com/becomingadatascientistadvice-pydatadc-shared-161012184823/95/becoming-a-data-scientist-advice-from-my-podcast-guests-55-638.jpg?cb=1476298295)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEZu7RSd0O3w"
   },
   "source": [
    "## Business Question --> Data Question --> Data Answer (for Supervised Learning)\n",
    "\n",
    "#### Francois Chollet, [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/README.md), Chapter 4: Fundamentals of machine learning, \"A universal workflow of machine learning\"\n",
    " \n",
    "> **1. Define the problem at hand and the data on which you’ll train.** Collect this data, or annotate it with labels if need be.\n",
    "\n",
    "> **2. Choose how you’ll measure success on your problem.** Which metrics will you monitor on your validation data?\n",
    "\n",
    "> **3. Determine your evaluation protocol:** hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
    "\n",
    "> **4. Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
    "\n",
    "> **5. Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
    "\n",
    "> **6. Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. \n",
    "\n",
    "> **Iterate on feature engineering: add new features, or remove features that don’t seem to be informative.** Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJhnVFGQRXS0"
   },
   "source": [
    "## Define the data on which you'll train / Add new features or remove features\n",
    "\n",
    "#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Data Representation in Scikit-Learn\n",
    "\n",
    "> The best way to think about data within Scikit-Learn is in terms of tables of data.\n",
    "\n",
    "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.02-samples-features.png)\n",
    "\n",
    "> The samples (i.e., rows) always refer to the individual objects described by the dataset. For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.\n",
    "\n",
    "> The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner. \n",
    "\n",
    "> The information can be thought of as a two-dimensional numerical array or matrix, which we will call the _features matrix._ By convention, this features matrix is often stored in a variable named `X`. The features matrix is assumed to be two-dimensional, with shape `[n_samples, n_features]`, and is most often contained in a NumPy array or a Pandas `DataFrame`, though some Scikit-Learn models also accept SciPy sparse matrices.\n",
    "\n",
    "> In addition to the feature matrix `X`, we also generally work with a label or target array, which by convention we will usually call `y`. The target array is usually one dimensional, with length `n_samples`, and is generally contained in a NumPy array or Pandas `Series`. \n",
    "\n",
    "> Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to _predict from the data:_ in statistical terms, it is the dependent variable.\n",
    "\n",
    "#### Google Developers, [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/#l) \n",
    "\n",
    "> Each example in a labeled dataset consists of one or more features and a label. \n",
    "\n",
    "> For instance, in a housing dataset, the features might include the number of bedrooms, the number of bathrooms, and the age of the house, while the label might be the house's price. \n",
    "\n",
    "> In a spam detection dataset, the features might include the subject line, the sender, and the email message itself, while the label would probably be either \"spam\" or \"not spam.\"\n",
    "\n",
    "#### Wikipedia, [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSZBdwNg1Vvj"
   },
   "source": [
    "## Determine evaluation protocol\n",
    "\n",
    "#### Sebastian Raschka, [Model Evaluation]( https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)\n",
    "> <img src=\"https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFAHa5dD3BQt"
   },
   "source": [
    "## Develop a first model that does better than a basic baseline\n",
    "\n",
    "### Why begin with baselines?\n",
    "\n",
    "[My mentor](https://www.linkedin.com/in/jason-sanchez-62093847/) [taught me](https://youtu.be/0GrciaGYzV0?t=40s):\n",
    "\n",
    ">***Your first goal should always, always, always be getting a generalized prediction as fast as possible.*** You shouldn't spend a lot of time trying to tune your model, trying to add features, trying to engineer features, until you've actually gotten one prediction, at least. \n",
    "\n",
    "> The reason why that's a really good thing is because then ***you'll set a benchmark*** for yourself, and you'll be able to directly see how much effort you put in translates to a better prediction. \n",
    "\n",
    "> What you'll find by working on many models: some effort you put in, actually has very little effect on how well your final model does at predicting new observations. Whereas some very easy changes actually have a lot of effect. And so you get better at allocating your time more effectively.\n",
    "\n",
    "My mentor's advice is echoed and elaborated in several sources:\n",
    "\n",
    "[Always start with a stupid model, no exceptions](https://blog.insightdatascience.com/always-start-with-a-stupid-model-no-exceptions-3a22314b9aaa)\n",
    "\n",
    "> Why start with a baseline? A baseline will take you less than 1/10th of the time, and could provide up to 90% of the results. A baseline puts a more complex model into context. Baselines are easy to deploy.\n",
    "\n",
    "[Measure Once, Cut Twice: Moving Towards Iteration in Data Science](https://blog.datarobot.com/measure-once-cut-twice-moving-towards-iteration-in-data-science)\n",
    "\n",
    "> The iterative approach in data science starts with emphasizing the importance of getting to a first model quickly, rather than starting with the variables and features. Once the first model is built, the work then steadily focuses on continual improvement.\n",
    "\n",
    "[*Data Science for Business*](https://books.google.com/books?id=4ZctAAAAQBAJ&pg=PT276), Chapter 7.3: Evaluation, Baseline Performance, and Implications for Investments in Data\n",
    "\n",
    "> *Consider carefully what would be a reasonable baseline against which to compare model performance.* This is important for the data science team in order to understand whether they indeed are improving performance, and is equally important for demonstrating to stakeholders that mining the data has added value.\n",
    "\n",
    "### What does baseline mean?\n",
    "\n",
    "Baseline is an overloaded term, as you can see in the links above. Baseline has multiple meanings:\n",
    "\n",
    "#### The score you'd get by guessing a single value\n",
    "\n",
    "> A baseline for classification can be the most common class in the training dataset.\n",
    "\n",
    "> A baseline for regression can be the mean of the training labels. —[Will Koehrsen](https://twitter.com/koehrsen_will/status/1088863527778111488)\n",
    "\n",
    "#### The score you'd get by guessing in a more granular way\n",
    "\n",
    "> A baseline for time-series regressions can be the value from the previous timestep.\n",
    "\n",
    "#### Fast, first models that beat guessing\n",
    "\n",
    "What my mentor was talking about.\n",
    "\n",
    "#### Complete, tuned \"simpler\" model\n",
    "\n",
    "Can be simpler mathematically and computationally. For example, Logistic Regression versus Deep Learning.\n",
    "\n",
    "Or can be simpler for the data scientist, with less work. For example, a model with less feature engineering versus a model with more feature engineering.\n",
    "\n",
    "#### Minimum performance that \"matters\"\n",
    "\n",
    "To go to production and get business value.\n",
    "\n",
    "#### Human-level performance \n",
    "\n",
    "Your goal may to be match, or nearly match, human performance, but with better speed, cost, or consistency.\n",
    "\n",
    "Or your goal may to be exceed human performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOW2hsiDBIk9"
   },
   "source": [
    "## Use scikit-learn to fit a model\n",
    "\n",
    "#### Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "> Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n",
    "\n",
    "> 1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn. \n",
    "> 2. Choose model hyperparameters by instantiating this class with desired values. \n",
    "> 3. Arrange data into a features matrix and target vector following the discussion above.\n",
    "> 4. Fit the model to your data by calling the `fit()` method of the model instance.\n",
    "> 5. Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1hPspa5mkWT"
   },
   "source": [
    "# Project\n",
    "\n",
    "### Predict presidential election voting, doing linear regression with two features\n",
    "\n",
    "#### Douglas Hibbs, [Background Information on the ‘Bread and Peace’ Model of Voting in Postwar US Presidential Elections](https://douglas-hibbs.com/background-information-on-bread-and-peace-voting-in-us-presidential-elections/)\n",
    "\n",
    "> According to the ‘Bread and Peace’ model, postwar US presidential elections can for the most part be interpreted as a sequence of referendums on the incumbent party’s record during its four-year mandate period. \n",
    "\n",
    "> In fact aggregate two-party vote shares going to candidates of the party holding the presidency during the postwar era are well explained by just two fundamental determinants:\n",
    "\n",
    "> (1) Positively by weighted-average growth of per capita real disposable personal income over the term.\n",
    "\n",
    "> (2) Negatively by cumulative US military fatalities (scaled to population) owing to unprovoked, hostile deployments of American armed forces in foreign wars.\n",
    "\n",
    "![](https://douglas-hibbs.com/wp-content/uploads/2013/08/BP1v4c2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbqeAzl9TK_y"
   },
   "source": [
    "## Define the data on which you'll train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5Cl5iTHXwA7"
   },
   "source": [
    "### Load data\n",
    "\n",
    "#### Sources\n",
    "- 1952-2012: Douglas Hibbs, [2014 lecture at Deakin University Melbourne](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 40\n",
    "- 2016, Vote Share: [The American Presidency Project](https://www.presidency.ucsb.edu/statistics/elections)\n",
    "- 2016, Recent Growth in Personal Incomes: [The 2016 election economy: the \"Bread and Peace\" model final forecast](https://angrybearblog.com/2016/11/the-2016-election-economy-the-bread-and-peace-model-final-forecast.html)\n",
    "- 2016, US Military Fatalities: Assumption that Afghanistan War fatalities in 2012-16 occured at the same rate as 2008-12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEeVFTa0VWDE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['Year','Incumbent Party Candidate','Other Candidate','Incumbent Party Vote Share']\n",
    "\n",
    "data = [[1952,\"Stevenson\",\"Eisenhower\",44.6],\n",
    "        [1956,\"Eisenhower\",\"Stevenson\",57.76],\n",
    "        [1960,\"Nixon\",\"Kennedy\",49.91],\n",
    "        [1964,\"Johnson\",\"Goldwater\",61.34],\n",
    "        [1968,\"Humphrey\",\"Nixon\",49.60],\n",
    "        [1972,\"Nixon\",\"McGovern\",61.79],\n",
    "        [1976,\"Ford\",\"Carter\",48.95],\n",
    "        [1980,\"Carter\",\"Reagan\",44.70],\n",
    "        [1984,\"Reagan\",\"Mondale\",59.17],\n",
    "        [1988,\"Bush, Sr.\",\"Dukakis\",53.94],\n",
    "        [1992,\"Bush, Sr.\",\"Clinton\",46.55],\n",
    "        [1996,\"Clinton\",\"Dole\",54.74],\n",
    "        [2000,\"Gore\",\"Bush, Jr.\",50.27],\n",
    "        [2004,\"Bush, Jr.\",\"Kerry\",51.24],\n",
    "        [2008,\"McCain\",\"Obama\",46.32],\n",
    "        [2012,\"Obama\",\"Romney\",52.00], \n",
    "        [2016,\"Clinton\",\"Trump\",48.2]]\n",
    "        \n",
    "votes = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNKTk8bV7c7U"
   },
   "outputs": [],
   "source": [
    "columns = ['Year','Average Recent Growth in Personal Incomes']\n",
    "\n",
    "data = [[1952,2.40],\n",
    "        [1956,2.89],\n",
    "        [1960, .85],\n",
    "        [1964,4.21],\n",
    "        [1968,3.02],\n",
    "        [1972,3.62],\n",
    "        [1976,1.08],\n",
    "        [1980,-.39],\n",
    "        [1984,3.86],\n",
    "        [1988,2.27],\n",
    "        [1992, .38],\n",
    "        [1996,1.04],\n",
    "        [2000,2.36],\n",
    "        [2004,1.72],\n",
    "        [2008, .10],\n",
    "        [2012, .95], \n",
    "        [2016, .10]]\n",
    "        \n",
    "growth = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxyoxOLQl7VX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fatalities denotes the cumulative number of American military\n",
    "fatalities per millions of US population the in Korea, Vietnam,\n",
    "Iraq and Afghanistan wars during the presidential terms\n",
    "preceding the 1952, 1964, 1968, 1976 and 2004, 2008 and\n",
    "2012 elections.\n",
    "\n",
    "http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf\n",
    "\"\"\"\n",
    "\n",
    "columns = ['Year','US Military Fatalities per Million']\n",
    "\n",
    "data = [[1952,190],\n",
    "        [1956,  0],\n",
    "        [1960,  0],\n",
    "        [1964,  1],\n",
    "        [1968,146],\n",
    "        [1972,  0],\n",
    "        [1976,  2],\n",
    "        [1980,  0],\n",
    "        [1984,  0],\n",
    "        [1988,  0],\n",
    "        [1992,  0],\n",
    "        [1996,  0],\n",
    "        [2000,  0],\n",
    "        [2004,  4],\n",
    "        [2008, 14],\n",
    "        [2012,  5], \n",
    "        [2016,  5]]\n",
    "        \n",
    "deaths = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXirDjeR73Bm"
   },
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8TWFDN_QZ_DM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMmPjCEWTXFn"
   },
   "source": [
    "## Begin with baselines for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x9aec8nw-Ybh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3hDX7yUTbix"
   },
   "source": [
    "## Use scikit-learn for linear regression, with 1 feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFWAop61CgCq"
   },
   "source": [
    "Follow the process from Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "### Choose a class of model by importing the appropriate estimator class from Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMbOVWEDCfmO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vATSdu5oD5NQ"
   },
   "source": [
    "### Choose model hyperparameters by instantiating this class with desired values\n",
    "\n",
    "Refer to scikit-learn documentation to see what model hyperparameters you can choose. For example: [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CexmSzauEBnu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEsa7jtHC0L5"
   },
   "source": [
    "### Arrange data into X features matrix and y target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euIG2-5P_sdZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8s3-WYWEKxN"
   },
   "source": [
    "### Fit the model to your data by calling the `fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTLnEzwUENb5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HynZZRL7ESvx"
   },
   "source": [
    "### Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mCtwohVKERED"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6l-WbivHIwHh"
   },
   "source": [
    "## Use regression metric: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5G7hDzGG0B0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfuI9wu5FvXu"
   },
   "outputs": [],
   "source": [
    "df['Absolute Error'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_KGM3LOHyrW"
   },
   "source": [
    "## Use scikit-learn for linear regression, with 2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_79qOeAH2ZU"
   },
   "source": [
    "Follow the process from Jake VanderPlas, [Python Data Science Handbook, Chapter 5.2, Introducing Scikit-Learn](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html), Scikit-Learn's Estimator API\n",
    "\n",
    "### Choose a class of model by importing the appropriate estimator class from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E26qduGiH2_y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--7julXYH3oC"
   },
   "source": [
    "### Choose model hyperparameters by instantiating this class with desired values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkkoMxbsIXLR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJNRFvK9IeWU"
   },
   "source": [
    "### Arrange data into X features matrix and y target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2290BJszIgrb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOa5Uj4jIjDR"
   },
   "source": [
    "### Fit the model to your data by calling the `fit()` method of the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZAUSsY0IjWa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GEyW2B3Imr2"
   },
   "source": [
    "### Apply the Model to new data: For supervised learning, often we predict labels for unknown data using the `predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ubKZVRJInLV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDbG8jreI8Ip"
   },
   "source": [
    "## Use regression metric: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCs--47RI-He"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZp_TyR9Tg9A"
   },
   "source": [
    "## Do leave-one-out cross-validation\n",
    "\n",
    "[Nate Silver's post on economic elections models](https://fivethirtyeight.com/features/what-do-economic-models-really-tell-us-about-elections/) discusses out-of-sample testing.\n",
    "\n",
    "[Sebastian Raschka's chart](https://sebastianraschka.com/images/blog/2018/model-evaluation-selection-part4/model-eval-conclusions.jpg) shows that \"leave-one-out cross-validation\" is an option for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avlXCUIAI-lL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxbxANM-JHdv"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "#### Predict presidential election voting, with two features you choose!\n",
    "- Start a new notebook.\n",
    "- You may reuse one of the features from the \"Bread & Peace\" model.\n",
    "- **Acquire data for at least one new feature.** The links below may help!\n",
    "- Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "#### Why I'm asking you to acquire data for at least one new [feature](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
    "\n",
    "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. \n",
    "\n",
    "#### You can search [FRED (Federal Reserve Bank of St. Louis)](https://fred.stlouisfed.org/) for these keywords\n",
    "- real disposable income change annual\n",
    "- gdp change annual\n",
    "- unemployment \n",
    "\n",
    "#### Go to [BEA (Bureau of Economic Analysis)](https://apps.bea.gov/itable/) and follow these steps\n",
    "- National Data - GDP & Personal Income\n",
    "- Begin using the data\n",
    "- Section 1 - DOMESTIC PRODUCT AND INCOME\n",
    "- Table 1.17.1. Percent Change From Preceding Period in Real Gross Domestic Product, Real Gross Domestic Income, and Other Major NIPA Aggregates\n",
    "- Modify\n",
    "  - First Year: 1947\n",
    "  - Last Year: 2018\n",
    "  - Series: Annual\n",
    "  - Refresh Table\n",
    "- Download\n",
    "\n",
    "#### Go to Wikipedia, [United States military casualties of war, Wars ranked by total number of U.S. military deaths](https://en.wikipedia.org/wiki/United_States_military_casualties_of_war#Wars_ranked_by_total_number_of_U.S._military_deaths)\n",
    "- You can try this tutorial to scrape data from HTML tables: [Quick Tip: The easiest way to grab data out of a web page in Python](https://medium.com/@ageitgey/quick-tip-the-easiest-way-to-grab-data-out-of-a-web-page-in-python-7153cecfca58)\n",
    "\n",
    "#### Read more about economic features to predict elections\n",
    "- [Which Economic Indicators Best Predict Presidential Elections?](https://fivethirtyeight.blogs.nytimes.com/2011/11/18/which-economic-indicators-best-predict-presidential-elections/)\n",
    "- [What stat best gets at the question, \"Are you better off now than you were a year ago?\"](https://www.theatlantic.com/business/archive/2010/11/the-most-important-economic-indicator-in-midterm-elections/65505/)\n",
    "- [Time for change model](https://pollyvote.com/en/components/econometric-models/time-for-change-model/)\n",
    "\n",
    "\n",
    "#### You can try for a \"serious\" model or a \"spurious\" model. Here are more data sources you can try\n",
    "- [Tyler Vigen, Spurious Correlations, Discover a Correlation](https://tylervigen.com/discover)\n",
    "- [CDC (Centers for Disease Control), Compressed Mortality data](https://wonder.cdc.gov/mortSQL.html)\n",
    "- [Data Is Plural](https://tinyletter.com/data-is-plural)\n",
    "- [Gapminder](https://github.com/open-numbers/ddf--gapminder--systema_globalis/)\n",
    "- [Campaign Finance Institute, Historical Stats](http://www.cfinst.org/data/historicalstats.aspx)\n",
    "- Or find your own data and features to try!\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "doing_linear_regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
